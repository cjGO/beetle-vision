{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO track history of tuner models\n",
    "#https://github.com/keras-team/keras-tuner/issues/120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras_preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE - I am working in an nvidia-docker image with official tensorflow docker.\n",
    "# I had to add this line AND 'trust' the notebook on top-right. Otherwise an ugly\n",
    "# cudNN error is present.\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generators(image_size,\n",
    "                    top6_train,\n",
    "                    top6_valid,\n",
    "                    y_target,\n",
    "                    batch_size=1):\n",
    "    \n",
    "    datagen=ImageDataGenerator(rescale=1./255)\n",
    "    train_generator=datagen.flow_from_dataframe(dataframe=top6_train,\n",
    "                                            directory=\"/tf/beetleData/images\",\n",
    "                                            x_col=\"Filename:\", y_col=y_target,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            target_size=image_size,\n",
    "                                            batch_size=batch_size)\n",
    "    validation_generator=datagen.flow_from_dataframe(dataframe=top6_valid,\n",
    "                                            directory=\"/tf/beetleData/images\",\n",
    "                                            x_col=\"Filename:\", y_col= y_target,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            target_size=image_size,\n",
    "                                            batch_size=batch_size,\n",
    "                                                    shuffle=False)\n",
    "    \n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Image augmentation block\n",
    "    #x = data_augmentation(inputs)\n",
    "\n",
    "    # Entry block\n",
    "    #x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(inpu)\n",
    "        # This is done already because we used an ImageDataGenerator\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [128, 256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_class(model,\n",
    "                epochs=10,\n",
    "               label_smooth=False,\n",
    "               class_weights=True,\n",
    "               ):\n",
    "    from sklearn.utils import class_weight\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\"/tf/beetleData/fam_10_58x58.h5\",\n",
    "                                       save_best_only = True),\n",
    "    ]\n",
    "    \n",
    "    if label_smooth:\n",
    "        loss = keras.losses.CategoricalCrossentropy(label_smoothing=label_smooth)\n",
    "    else:\n",
    "        loss = 'categorical_crossentropy'\n",
    "    \n",
    "    #LABEL_SMOOTHING BLOCK\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=loss,\n",
    "        metrics=[\"accuracy\"],\n",
    ")\n",
    "    \n",
    "    y_train = train_generator.classes\n",
    "    \n",
    "    \n",
    "    #CLASS_WEIGHTS BLOCK\n",
    "    if class_weights:\n",
    "        c_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                     np.unique(y_train),\n",
    "                                                     np.array(y_train))\n",
    "        class_weight_dict = dict(enumerate(c_weights))\n",
    "        \n",
    "        \n",
    "        \n",
    "    #Fit the model - based on conditions\n",
    "        history58x58 = model.fit(\n",
    "            train_generator,validation_data=validation_generator,epochs=epochs,callbacks = [callbacks]\n",
    "            #    train_generator,validation_data=validation_generator,epochs=epochs, callbacks=callbacks\n",
    "        )\n",
    "        return history58x58\n",
    "    else:\n",
    "        history58x58 = model.fit(\n",
    "        train_generator,validation_data=validation_generator,epochs=epochs ,callbacks = [callbacks]\n",
    "        #    train_generator,validation_data=validation_generator,epochs=epochs, callbacks=callbacks\n",
    "    )\n",
    "        return history58x58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvSplitter(dataframe, split_ratio):\n",
    "    \"\"\"\n",
    "    To use the data generator in keras with flow_from_dataset, you need a helper function to perform the train/test\n",
    "    split.\n",
    "    \n",
    "    Input: a pandas CSV/Table of all your data.\n",
    "    Output : the same dataframe, split into seperate training/validation dataframes, to be used in rest of pipeline\n",
    "    \"\"\"\n",
    "    dataframe = dataframe.sample(frac=1).reset_index(drop=True) #Shuffles DataFrame - for a lazy way to test/train split\n",
    "    validation_index = round(dataframe.shape[0] * split_ratio) # Gets row # of images in validation set \n",
    "    training_index = dataframe.shape[0] - validation_index # Gets row # of images in training set\n",
    "    train_valid_split = np.concatenate((np.ones(validation_index),np.zeros(training_index))) # creates train-valid column\n",
    "    dataframe['trainValid'] = train_valid_split # adds the splitting ID to the dataframe\n",
    "    train_df = dataframe[dataframe['trainValid'] == 0.0] #subsetting based on previously created test/train split col\n",
    "    valid_df = dataframe[dataframe['trainValid'] == 1.0]\n",
    "    \n",
    "    return train_df, valid_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beetle_labels(y_target = 'Family:',\n",
    "                      data_dir = '/tf/beetleData/images/',\n",
    "                      label_file = '/tf/beetleData/labels/master_rbi.csv',\n",
    "                      top_classes = 6):\n",
    "    \"\"\"\n",
    "    Read label file\n",
    "    Get images from Dorsal View\n",
    "    Take those images and use csvsplitter\n",
    "    Return train and test dataframes\n",
    "    \"\"\"\n",
    "    labels = pd.read_csv(label_file)\n",
    "    assert y_target in labels.columns, \"y_target should be a column in the label_file!\"\n",
    "    \n",
    "    data_dir = pathlib.Path('/tf/beetleData/images/')\n",
    "    dorsal_data = labels[labels[\"View:\"] == \"Dorsal\"] #filter images that are from dorsal view\n",
    "    dorsal_data[y_target].value_counts().sort_values().plot(kind = 'barh') # preview the distribution\n",
    "    \n",
    "    top6 = dorsal_data[y_target].value_counts(normalize=True)[:top_classes]\n",
    "    print(top6)\n",
    "    print('c1' + str(top6.shape))\n",
    "    top6 = top6.index.values # get top 6 most common classes.\n",
    "    top6_df = dorsal_data[dorsal_data[y_target].isin(top6)] #filter images from only top 6 classes\n",
    "    top6_train, top6_valid = csvSplitter(top6_df, .2)\n",
    "    #print(top6_train[y_target])\n",
    "    #print(top6_valid[y_target])\n",
    "    #failsafe -> make sure that no unique labels are exclusive to training or validation set\n",
    "    overlap_labels = (list(set(top6_train[y_target]) & set(top6_valid[y_target])))\n",
    "    top6_train = top6_train.loc[top6_train[y_target].isin(overlap_labels)]\n",
    "    print(top6_valid.shape)\n",
    "    top6_valid = top6_valid.loc[top6_valid[y_target].isin(overlap_labels)]\n",
    "    print(top6_valid.shape)\n",
    "    \n",
    "    #Add a failsafe - > make sure no unique target labels are exclusively in training or validation\n",
    "    print('loaded in training size of ' + str(top6_train.shape))\n",
    "    \n",
    "    return top6_train, top6_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scarabaeidae     0.492287\n",
      "Cerambycidae     0.241449\n",
      "Lucanidae        0.100604\n",
      "Buprestidae      0.069081\n",
      "Carabidae        0.048625\n",
      "Cicindelidae     0.030181\n",
      "Geotrupidae      0.007042\n",
      "Chrysomelidae    0.001677\n",
      "Silphidae        0.001677\n",
      "Curculionidae    0.001341\n",
      "Name: Family:, dtype: float64\n",
      "c1(10,)\n",
      "(593, 9)\n",
      "(593, 9)\n",
      "loaded in training size of (2371, 9)\n",
      "Found 2371 validated image filenames belonging to 10 classes.\n",
      "Found 593 validated image filenames belonging to 10 classes.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:71: FutureWarning: Pass classes=[0 1 2 3 4 5 6 7 8 9], y=[0 1 7 ... 8 7 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " logits and labels must be broadcastable: logits_size=[6,8] labels_size=[6,10]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits (defined at <ipython-input-19-1e351392746f>:39) ]] [Op:__inference_train_function_65696]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2ddc2f85f4e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#checkpoint_cb = tensorflow.keras.callbacks.ModelCheckPoint('my_keras_model.h5',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#model = make_model(input_shape=image_size + (3,), num_classes=num_classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_smooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-1e351392746f>\u001b[0m in \u001b[0;36mtrain_class\u001b[0;34m(model, epochs, label_smooth, class_weights)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#Fit the model - based on conditions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         history58x58 = model.fit(\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;31m#    train_generator,validation_data=validation_generator,epochs=epochs, callbacks=callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1088\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1089\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    764\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m   1837\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1913\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1915\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1916\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1917\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must be broadcastable: logits_size=[6,8] labels_size=[6,10]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits (defined at <ipython-input-19-1e351392746f>:39) ]] [Op:__inference_train_function_65696]\n\nFunction call stack:\ntrain_function\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_target='Family:'\n",
    "top_classes = 10 # attempts to get 100 classes. but may be less if the labels are exclusive to test/valid\n",
    "epochs = 50\n",
    "label_smooth=0.1\n",
    "image_size=(58,58)\n",
    "\n",
    "tr, va = get_beetle_labels(y_target=y_target,\n",
    "                      top_classes=top_classes) # top classes has to be oddly high (for any label\n",
    "                                        # other than the family label, unsure why\n",
    "\n",
    "assert sum(np.sort(va[y_target].unique()) == np.sort(tr[y_target].unique())) == len(va[y_target].unique()), \\\n",
    "    'Mismatched Labels?'\n",
    "train_generator,  validation_generator  = make_generators(image_size, tr, va,y_target,batch_size=6)\n",
    "class_labels = train_generator.class_indices\n",
    "num_classes = len(np.unique(train_generator.classes))\n",
    "#checkpoint_cb = tensorflow.keras.callbacks.ModelCheckPoint('my_keras_model.h5',\n",
    "model = make_model(input_shape=image_size + (3,), num_classes=num_classes)\n",
    "history = train_class(model, epochs=epochs,label_smooth=.1, class_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 43   0   0   0   0   0   0   0   0   0]\n",
      " [  0  36   0   0   0   0   0   0   0   0]\n",
      " [  0   0 144   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   1  17   0   0   0   0   0   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   2   0   2   0   0   0]\n",
      " [  0   0   0   0   0  57   0   0   0   0]\n",
      " [  0   0   0   0   0   0 287   0   0   0]\n",
      " [  0   0   1   0   0   0   1   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(validation_generator)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Buprestidae       1.00      1.00      1.00        43\n",
      "    Carabidae       1.00      1.00      1.00        36\n",
      " Cerambycidae       0.98      1.00      0.99       144\n",
      "Chrysomelidae       0.00      0.00      0.00         1\n",
      " Cicindelidae       0.00      0.00      0.00        18\n",
      "Curculionidae       0.00      0.00      0.00         1\n",
      "  Geotrupidae       0.01      0.50      0.01         4\n",
      "    Lucanidae       0.00      0.00      0.00        57\n",
      " Scarabaeidae       0.00      0.00      0.00       287\n",
      "    Silphidae       0.00      0.00      0.00         2\n",
      "\n",
      "     accuracy                           0.38       593\n",
      "    macro avg       0.30      0.35      0.30       593\n",
      " weighted avg       0.37      0.38      0.37       593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "target_names =  list(train_generator.class_indices.keys())\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history['val_accuracy']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "    input_shape = (100,100)\n",
    "    num_classes = 40\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape + (3,))\n",
    "    # Image augmentation block\n",
    "    #x = data_augmentation(inputs)\n",
    "\n",
    "    # Entry block\n",
    "    #x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(inpu)\n",
    "        # This is done already because we used an ImageDataGenerator\n",
    "    x = layers.Conv2D(hp.Choice('first_layer', values=[128,256,512]), 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Conv2D(hp.Choice('second_layer', values=[64,128,256,512]), 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(hp.Choice('dropout_final_layer',values=[0.05,0.02,0.0]))(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    \n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-3, 1e-4, 1e-5])),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='/tf/beetleData/',\n",
    "    project_name='helloworld4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_generator,\n",
    "             epochs=10,\n",
    "             validation_data= validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = build_model(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.tuner_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tuner.get_best_models(num_models=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[0].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].evaluate(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
