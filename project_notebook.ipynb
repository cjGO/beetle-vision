{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "\n",
    "class Projector:\n",
    "    def __init__(self):\n",
    "        self.num_steps                  = 10000\n",
    "        self.dlatent_avg_samples        = 10000\n",
    "        self.initial_learning_rate      = 0.1\n",
    "        self.initial_noise_factor       = 0.05\n",
    "        self.lr_rampdown_length         = 0.25\n",
    "        self.lr_rampup_length           = 0.05\n",
    "        self.noise_ramp_length          = 0.75\n",
    "        self.regularize_noise_weight    = 1e5\n",
    "        self.verbose                    = True\n",
    "\n",
    "        self._Gs                    = None\n",
    "        self._minibatch_size        = None\n",
    "        self._dlatent_avg           = None\n",
    "        self._dlatent_std           = None\n",
    "        self._noise_vars            = None\n",
    "        self._noise_init_op         = None\n",
    "        self._noise_normalize_op    = None\n",
    "        self._dlatents_var          = None\n",
    "        self._dlatent_noise_in      = None\n",
    "        self._dlatents_expr         = None\n",
    "        self._images_float_expr     = None\n",
    "        self._images_uint8_expr     = None\n",
    "        self._target_images_var     = None\n",
    "        self._lpips                 = None\n",
    "        self._dist                  = None\n",
    "        self._loss                  = None\n",
    "        self._reg_sizes             = None\n",
    "        self._lrate_in              = None\n",
    "        self._opt                   = None\n",
    "        self._opt_step              = None\n",
    "        self._cur_step              = None\n",
    "\n",
    "    def _info(self, *args):\n",
    "        if self.verbose:\n",
    "            print('Projector:', *args)\n",
    "\n",
    "    def set_network(self, Gs, dtype='float16'):\n",
    "        if Gs is None:\n",
    "            self._Gs = None\n",
    "            return\n",
    "        self._Gs = Gs.clone(randomize_noise=False, dtype=dtype, num_fp16_res=0, fused_modconv=True)\n",
    "\n",
    "        # Compute dlatent stats.\n",
    "        self._info(f'Computing W midpoint and stddev using {self.dlatent_avg_samples} samples...')\n",
    "        latent_samples = np.random.RandomState(123).randn(self.dlatent_avg_samples, *self._Gs.input_shapes[0][1:])\n",
    "        dlatent_samples = self._Gs.components.mapping.run(latent_samples, None)  # [N, L, C]\n",
    "        dlatent_samples = dlatent_samples[:, :1, :].astype(np.float32)           # [N, 1, C]\n",
    "        self._dlatent_avg = np.mean(dlatent_samples, axis=0, keepdims=True)      # [1, 1, C]\n",
    "        self._dlatent_std = (np.sum((dlatent_samples - self._dlatent_avg) ** 2) / self.dlatent_avg_samples) ** 0.5\n",
    "        self._info(f'std = {self._dlatent_std:g}')\n",
    "\n",
    "        # Setup noise inputs.\n",
    "        self._info('Setting up noise inputs...')\n",
    "        self._noise_vars = []\n",
    "        noise_init_ops = []\n",
    "        noise_normalize_ops = []\n",
    "        while True:\n",
    "            n = f'G_synthesis/noise{len(self._noise_vars)}'\n",
    "            if not n in self._Gs.vars:\n",
    "                break\n",
    "            v = self._Gs.vars[n]\n",
    "            self._noise_vars.append(v)\n",
    "            noise_init_ops.append(tf.assign(v, tf.random_normal(tf.shape(v), dtype=tf.float32)))\n",
    "            noise_mean = tf.reduce_mean(v)\n",
    "            noise_std = tf.reduce_mean((v - noise_mean)**2)**0.5\n",
    "            noise_normalize_ops.append(tf.assign(v, (v - noise_mean) / noise_std))\n",
    "        self._noise_init_op = tf.group(*noise_init_ops)\n",
    "        self._noise_normalize_op = tf.group(*noise_normalize_ops)\n",
    "\n",
    "        # Build image output graph.\n",
    "        self._info('Building image output graph...')\n",
    "        self._minibatch_size = 1\n",
    "        self._dlatents_var = tf.Variable(tf.zeros([self._minibatch_size] + list(self._dlatent_avg.shape[1:])), name='dlatents_var')\n",
    "        self._dlatent_noise_in = tf.placeholder(tf.float32, [], name='noise_in')\n",
    "        dlatents_noise = tf.random.normal(shape=self._dlatents_var.shape) * self._dlatent_noise_in\n",
    "        self._dlatents_expr = tf.tile(self._dlatents_var + dlatents_noise, [1, self._Gs.components.synthesis.input_shape[1], 1])\n",
    "        self._images_float_expr = tf.cast(self._Gs.components.synthesis.get_output_for(self._dlatents_expr), tf.float32)\n",
    "        self._images_uint8_expr = tflib.convert_images_to_uint8(self._images_float_expr, nchw_to_nhwc=True)\n",
    "\n",
    "        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
    "        proc_images_expr = (self._images_float_expr + 1) * (255 / 2)\n",
    "        sh = proc_images_expr.shape.as_list()\n",
    "        if sh[2] > 256:\n",
    "            factor = sh[2] // 256\n",
    "            proc_images_expr = tf.reduce_mean(tf.reshape(proc_images_expr, [-1, sh[1], sh[2] // factor, factor, sh[2] // factor, factor]), axis=[3,5])\n",
    "\n",
    "        # Build loss graph.\n",
    "        self._info('Building loss graph...')\n",
    "        self._target_images_var = tf.Variable(tf.zeros(proc_images_expr.shape), name='target_images_var')\n",
    "        if self._lpips is None:\n",
    "            with dnnlib.util.open_url('https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/metrics/vgg16_zhang_perceptual.pkl') as f:\n",
    "                self._lpips = pickle.load(f)\n",
    "        self._dist = self._lpips.get_output_for(proc_images_expr, self._target_images_var)\n",
    "        self._loss = tf.reduce_sum(self._dist)\n",
    "\n",
    "        # Build noise regularization graph.\n",
    "        self._info('Building noise regularization graph...')\n",
    "        reg_loss = 0.0\n",
    "        for v in self._noise_vars:\n",
    "            sz = v.shape[2]\n",
    "            while True:\n",
    "                reg_loss += tf.reduce_mean(v * tf.roll(v, shift=1, axis=3))**2 + tf.reduce_mean(v * tf.roll(v, shift=1, axis=2))**2\n",
    "                if sz <= 8:\n",
    "                    break # Small enough already\n",
    "                v = tf.reshape(v, [1, 1, sz//2, 2, sz//2, 2]) # Downscale\n",
    "                v = tf.reduce_mean(v, axis=[3, 5])\n",
    "                sz = sz // 2\n",
    "        self._loss += reg_loss * self.regularize_noise_weight\n",
    "\n",
    "        # Setup optimizer.\n",
    "        self._info('Setting up optimizer...')\n",
    "        self._lrate_in = tf.placeholder(tf.float32, [], name='lrate_in')\n",
    "        self._opt = tflib.Optimizer(learning_rate=self._lrate_in)\n",
    "        self._opt.register_gradients(self._loss, [self._dlatents_var] + self._noise_vars)\n",
    "        self._opt_step = self._opt.apply_updates()\n",
    "\n",
    "    def start(self, target_images):\n",
    "        assert self._Gs is not None\n",
    "\n",
    "        # Prepare target images.\n",
    "        self._info('Preparing target images...')\n",
    "        target_images = np.asarray(target_images, dtype='float32')\n",
    "        target_images = (target_images + 1) * (255 / 2)\n",
    "        sh = target_images.shape\n",
    "        assert sh[0] == self._minibatch_size\n",
    "        if sh[2] > self._target_images_var.shape[2]:\n",
    "            factor = sh[2] // self._target_images_var.shape[2]\n",
    "            target_images = np.reshape(target_images, [-1, sh[1], sh[2] // factor, factor, sh[3] // factor, factor]).mean((3, 5))\n",
    "\n",
    "        # Initialize optimization state.\n",
    "        self._info('Initializing optimization state...')\n",
    "        dlatents = np.tile(self._dlatent_avg, [self._minibatch_size, 1, 1])\n",
    "        tflib.set_vars({self._target_images_var: target_images, self._dlatents_var: dlatents})\n",
    "        tflib.run(self._noise_init_op)\n",
    "        self._opt.reset_optimizer_state()\n",
    "        self._cur_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        assert self._cur_step is not None\n",
    "        if self._cur_step >= self.num_steps:\n",
    "            return 0, 0\n",
    "\n",
    "        # Choose hyperparameters.\n",
    "        t = self._cur_step / self.num_steps\n",
    "        dlatent_noise = self._dlatent_std * self.initial_noise_factor * max(0.0, 1.0 - t / self.noise_ramp_length) ** 2\n",
    "        lr_ramp = min(1.0, (1.0 - t) / self.lr_rampdown_length)\n",
    "        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
    "        lr_ramp = lr_ramp * min(1.0, t / self.lr_rampup_length)\n",
    "        learning_rate = self.initial_learning_rate * lr_ramp\n",
    "\n",
    "        # Execute optimization step.\n",
    "        feed_dict = {self._dlatent_noise_in: dlatent_noise, self._lrate_in: learning_rate}\n",
    "        _, dist_value, loss_value = tflib.run([self._opt_step, self._dist, self._loss], feed_dict)\n",
    "        tflib.run(self._noise_normalize_op)\n",
    "        self._cur_step += 1\n",
    "        return dist_value, loss_value\n",
    "\n",
    "    @property\n",
    "    def cur_step(self):\n",
    "        return self._cur_step\n",
    "\n",
    "    @property\n",
    "    def dlatents(self):\n",
    "        return tflib.run(self._dlatents_expr, {self._dlatent_noise_in: 0})\n",
    "\n",
    "    @property\n",
    "    def noises(self):\n",
    "        return tflib.run(self._noise_vars)\n",
    "\n",
    "    @property\n",
    "    def images_float(self):\n",
    "        return tflib.run(self._images_float_expr, {self._dlatent_noise_in: 0})\n",
    "\n",
    "    @property\n",
    "    def images_uint8(self):\n",
    "        return tflib.run(self._images_uint8_expr, {self._dlatent_noise_in: 0})\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def project(network_pkl: str, target_fname: str, outdir: str, save_video: bool, seed: int):\n",
    "    # Load networks.\n",
    "    tflib.init_tf({'rnd.np_random_seed': seed})\n",
    "    print('Loading networks from \"%s\"...' % network_pkl)\n",
    "    with dnnlib.util.open_url(network_pkl) as fp:\n",
    "        _G, _D, Gs = pickle.load(fp)\n",
    "\n",
    "    # Load target image.\n",
    "    target_pil = PIL.Image.open(target_fname)\n",
    "    w, h = target_pil.size\n",
    "    s = min(w, h)\n",
    "    target_pil = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n",
    "    target_pil= target_pil.convert('RGB')\n",
    "    target_pil = target_pil.resize((Gs.output_shape[3], Gs.output_shape[2]), PIL.Image.ANTIALIAS)\n",
    "    target_uint8 = np.array(target_pil, dtype=np.uint8)\n",
    "    target_float = target_uint8.astype(np.float32).transpose([2, 0, 1]) * (2 / 255) - 1\n",
    "\n",
    "    # Initialize projector.\n",
    "    proj = Projector()\n",
    "    proj.set_network(Gs)\n",
    "    proj.start([target_float])\n",
    "\n",
    "    # Setup output directory.\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    target_pil.save(f'{outdir}/target.png')\n",
    "    writer = None\n",
    "    if save_video:\n",
    "        writer = imageio.get_writer(f'{outdir}/proj.mp4', mode='I', fps=60, codec='libx264', bitrate='16M')\n",
    "\n",
    "    # Run projector.\n",
    "    with tqdm.trange(proj.num_steps) as t:\n",
    "        for step in t:\n",
    "            assert step == proj.cur_step\n",
    "            if writer is not None:\n",
    "                writer.append_data(np.concatenate([target_uint8, proj.images_uint8[0]], axis=1))\n",
    "            dist, loss = proj.step()\n",
    "            t.set_postfix(dist=f'{dist[0]:.4f}', loss=f'{loss:.2f}')\n",
    "\n",
    "    # Save results.\n",
    "    PIL.Image.fromarray(proj.images_uint8[0], 'RGB').save(f'{outdir}/proj.png')\n",
    "    np.savez(f'{outdir}/dlatents.npz', dlatents=proj.dlatents)\n",
    "    if writer is not None:\n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading networks from \"/workspace/Downloads/network-snapshot-000144.pkl\"...\n",
      "Projector: Computing W midpoint and stddev using 10000 samples...\n",
      "Projector: std = 18.9308\n",
      "Projector: Setting up noise inputs...\n",
      "Projector: Building image output graph...\n",
      "Projector: Building loss graph...\n",
      "Projector: Building noise regularization graph...\n",
      "Projector: Setting up optimizer...\n",
      "Projector: Preparing target images...\n",
      "Projector: Initializing optimization state...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [41:13<00:00,  4.04it/s, dist=0.0293, loss=0.03]  \n"
     ]
    }
   ],
   "source": [
    "project(network_pkl='/workspace/Downloads/network-snapshot-000144.pkl',target_fname='./rbi_2366.jpg', outdir='./projectOut',save_video='./', seed=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
