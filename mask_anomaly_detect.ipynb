{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal for this notebook is to create an autoencoder to sort out bad segmentations. Some of the 4000 images had odd artifacts. There are a few ways to filter out these poor segmentations. Here, we will try to use an autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largely from : https://www.pyimagesearch.com/2020/03/02/anomaly-detection-with-keras-tensorflow-and-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "#tf.test.is_gpu_available()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "import numpy as np\n",
    "class ConvAutoencoder:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, filters=(32, 64), latentDim=16):\n",
    "\t\t# initialize the input shape to be \"channels last\" along with\n",
    "\t\t# the channels dimension itself\n",
    "\t\t# channels dimension itself\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\t\t# define the input to the encoder\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tx = inputs\n",
    "\t\t# loop over the number of filters\n",
    "\t\tfor f in filters:\n",
    "\t\t\t# apply a CONV => RELU => BN operation\n",
    "\t\t\tx = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\t# flatten the network and then construct our latent vector\n",
    "\t\tvolumeSize = K.int_shape(x)\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tlatent = Dense(latentDim)(x)\n",
    "\t\t# build the encoder model\n",
    "\t\tencoder = Model(inputs, latent, name=\"encoder\")\n",
    "\t\t# start building the decoder model which will accept the\n",
    "\t\t# output of the encoder as its inputs\n",
    "\t\tlatentInputs = Input(shape=(latentDim,))\n",
    "\t\tx = Dense(np.prod(volumeSize[1:]))(latentInputs)\n",
    "\t\tx = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
    "\t\t# loop over our number of filters again, but this time in\n",
    "\t\t# reverse order\n",
    "\t\tfor f in filters[::-1]:\n",
    "\t\t\t# apply a CONV_TRANSPOSE => RELU => BN operation\n",
    "\t\t\tx = Conv2DTranspose(f, (3, 3), strides=2,\n",
    "\t\t\t\tpadding=\"same\")(x)\n",
    "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\t# apply a single CONV_TRANSPOSE layer used to recover the\n",
    "\t\t# original depth of the image\n",
    "\t\tx = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
    "\t\toutputs = Activation(\"sigmoid\")(x)\n",
    "\t\t# build the decoder model\n",
    "\t\tdecoder = Model(latentInputs, outputs, name=\"decoder\")\n",
    "\t\t# our autoencoder is the encoder + decoder\n",
    "\t\tautoencoder = Model(inputs, decoder(encoder(inputs)),\n",
    "\t\t\tname=\"autoencoder\")\n",
    "\t\t# return a 3-tuple of the encoder, decoder, and autoencoder\n",
    "\t\treturn (encoder, decoder, autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "#from pyimagesearch.convautoencoder import ConvAutoencoder\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "\n",
    "(encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "autoencoder.compile(loss=\"mse\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "((trainX, trainY), (testX, testY)) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 7216.8218 - val_loss: 7342.8857\n",
      "Epoch 2/20\n",
      "1316/1875 [====================>.........] - ETA: 1s - loss: 7214.9233"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ecdb7793c45f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \tbatch_size=BS)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# use the convolutional autoencoder to make predictions on the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# testing images, construct the visualization, and then save it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1088\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1089\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    764\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    791\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2809\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2810\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2811\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2812\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3153\u001b[0m           *args, **kwargs)\n\u001b[1;32m   3154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3155\u001b[0;31m     \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_cache_key\u001b[0;34m(self, args, kwargs, include_tensor_ranks_only)\u001b[0m\n\u001b[1;32m   2990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2991\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2992\u001b[0;31m     \u001b[0mdefault_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2993\u001b[0m     \u001b[0;31m# TODO(b/117617952): The current distribution strategy will affect graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m     \u001b[0;31m# building (e.g. accessing different variables from different devices) and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5985\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m   \"\"\"\n\u001b[0;32m-> 5987\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_default_graph_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#rain the convolutional autoencoder\n",
    "H = autoencoder.fit(\n",
    "\ttrainX, trainX,\n",
    "\tvalidation_data=(testX, testX),\n",
    "\tepochs=EPOCHS,\n",
    "\tbatch_size=BS)\n",
    "# use the convolutional autoencoder to make predictions on the\n",
    "# testing images, construct the visualization, and then save it\n",
    "# to disk\n",
    "print(\"[INFO] making predictions...\")\n",
    "decoded = autoencoder.predict(testX)\n",
    "vis = visualize_predictions(decoded, testX)\n",
    "cv2.imwrite(args[\"vis\"], vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 450, 3)\n",
      "(1, 100, 100)\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n"
     ]
    }
   ],
   "source": [
    "#loop through binary mask images\n",
    "import os\n",
    "binary_dir = '/tf/beetleData/images/binaryMask/'\n",
    "count = 0\n",
    "for i in os.listdir(binary_dir):\n",
    "    if 'jpg' in i:\n",
    "        if count == 0:        \n",
    "            img_in = cv2.imread(binary_dir+i)\n",
    "            print(img_in.shape)\n",
    "            single_channels = img_in[:,:,0]\n",
    "            single_channels = cv2.resize(single_channels,(100,100))\n",
    "            single_channels = np.expand_dims(single_channels,axis=0)\n",
    "\n",
    "            print(single_channel.shape)\n",
    "            count+=1\n",
    "        else:\n",
    "            img_in = cv2.imread(binary_dir+i)\n",
    "            #print(img_in.shape)\n",
    "            single_channel = img_in[:,:,0]\n",
    "            single_channel = cv2.resize(single_channel,(100,100))\n",
    "            single_channel = np.expand_dims(single_channel,axis=0)\n",
    "            #print('single channel ' + str(single_channel.shape))\n",
    "            #print('channels ' + str(single_channels.shape))\n",
    "            single_channels = np.concatenate((single_channels,single_channel),axis=0)\n",
    "            count+=1\n",
    "            if count % 50 == 0:\n",
    "                print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4074, 100, 100)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_channels.shape\n",
    "# Now we have our MNIST-like data structure. Let's try to use it to train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytrainX, mytestX = train_test_split(single_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3055, 100, 100)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytrainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1019, 100, 100)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       ...,\n",
       "       [255, 255, 255, ...,   0,   0,   0],\n",
       "       [255, 255, 255, ...,   0,   0,   0],\n",
       "       [255, 255, 255, ...,   0,   0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytrainX[0,:,:][1:50,1:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "         18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "        253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "        253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "        198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "         11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "          2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "         70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "        225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "        240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "        229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "        253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "        253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "         80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0]], dtype=uint8)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0,:,:][1:50,1:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, filters=(32, 64), latentDim=16):\n",
    "\t\t# initialize the input shape to be \"channels last\" along with\n",
    "\t\t# the channels dimension itself\n",
    "\t\t# channels dimension itself\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\t\t# define the input to the encoder\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tx = inputs\n",
    "\t\t# loop over the number of filters\n",
    "\t\tfor f in filters:\n",
    "\t\t\t# apply a CONV => RELU => BN operation\n",
    "\t\t\tx = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\t# flatten the network and then construct our latent vector\n",
    "\t\tvolumeSize = K.int_shape(x)\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tlatent = Dense(latentDim)(x)\n",
    "\t\t# build the encoder model\n",
    "\t\tencoder = Model(inputs, latent, name=\"encoder\")\n",
    "\t\t# start building the decoder model which will accept the\n",
    "\t\t# output of the encoder as its inputs\n",
    "\t\tlatentInputs = Input(shape=(latentDim,))\n",
    "\t\tx = Dense(np.prod(volumeSize[1:]))(latentInputs)\n",
    "\t\tx = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
    "\t\t# loop over our number of filters again, but this time in\n",
    "\t\t# reverse order\n",
    "\t\tfor f in filters[::-1]:\n",
    "\t\t\t# apply a CONV_TRANSPOSE => RELU => BN operation\n",
    "\t\t\tx = Conv2DTranspose(f, (3, 3), strides=2,\n",
    "\t\t\t\tpadding=\"same\")(x)\n",
    "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\t# apply a single CONV_TRANSPOSE layer used to recover the\n",
    "\t\t# original depth of the image\n",
    "\t\tx = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
    "\t\toutputs = Activation(\"sigmoid\")(x)\n",
    "\t\t# build the decoder model\n",
    "\t\tdecoder = Model(latentInputs, outputs, name=\"decoder\")\n",
    "\t\t# our autoencoder is the encoder + decoder\n",
    "\t\tautoencoder = Model(inputs, decoder(encoder(inputs)),\n",
    "\t\t\tname=\"autoencoder\")\n",
    "\t\t# return a 3-tuple of the encoder, decoder, and autoencoder\n",
    "\t\treturn (encoder, decoder, autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "\n",
    "(encoder, decoder, autoencoder) = ConvAutoencoder.build(100, 100, 1)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "autoencoder.compile(loss=\"mae\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1528/1528 [==============================] - 6s 4ms/step - loss: 194.1493 - val_loss: 194.4836\n",
      "Epoch 2/5\n",
      "1528/1528 [==============================] - 6s 4ms/step - loss: 194.1358 - val_loss: 194.4814\n",
      "Epoch 3/5\n",
      "1528/1528 [==============================] - 6s 4ms/step - loss: 194.1330 - val_loss: 194.4788\n",
      "Epoch 4/5\n",
      "1528/1528 [==============================] - 6s 4ms/step - loss: 194.1314 - val_loss: 194.4778\n",
      "Epoch 5/5\n",
      "1528/1528 [==============================] - 6s 4ms/step - loss: 194.1304 - val_loss: 194.4769\n"
     ]
    }
   ],
   "source": [
    "#rain the convolutional autoencoder\n",
    "H = autoencoder.fit(\n",
    "\tmytrainX, mytrainX,\n",
    "\tvalidation_data=(mytestX, mytestX),\n",
    "\tepochs=5,\n",
    "\tbatch_size=2,\n",
    "    verbose=1)\n",
    "# use the convolutional autoencoder to make predictions on the\n",
    "# testing images, construct the visualization, and then save it\n",
    "# to disk\n",
    "# print(\"[INFO] making predictions...\")\n",
    "# decoded = autoencoder.predict(testX)\n",
    "# vis = visualize_predictions(decoded, testX)\n",
    "# cv2.imwrite(args[\"vis\"], vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
